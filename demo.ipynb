{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "2TCtWp8G7q15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SguHEpG5-Q_i",
        "outputId": "83ee2c95-6716-4a56-bee7-902765e6f888"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mne\n",
        "#!pip install braindecode\n",
        "!pip install git+https://github.com/TNTLFreiburg/braindecode.git"
      ],
      "metadata": {
        "id": "5ik337LEW6V_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "from braindecode.datautil.iterators import _compute_start_stop_block_inds\n",
        "\n",
        "class CroppedDiagnosisMonitor(object):\n",
        "    \"\"\"\n",
        "    Compute trialwise misclasses from predictions for crops for non-dense predictions.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_time_length: int\n",
        "        Temporal length of one input to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_time_length, n_preds_per_input):\n",
        "        self.input_time_length = input_time_length\n",
        "        self.n_preds_per_input = n_preds_per_input\n",
        "\n",
        "    def monitor_epoch(self, ):\n",
        "        return\n",
        "\n",
        "    def monitor_set(self, setname, all_preds, all_losses,\n",
        "                    all_batch_sizes, all_targets, dataset):\n",
        "        \"\"\"Assuming one hot encoding for now\"\"\"\n",
        "        preds_per_trial = compute_preds_per_trial(\n",
        "            all_preds, dataset, input_time_length=self.input_time_length,\n",
        "            n_stride=self.n_preds_per_input)\n",
        "\n",
        "        mean_preds_per_trial = [np.mean(preds, axis=(0, 2)) for preds in\n",
        "                                preds_per_trial]\n",
        "        mean_preds_per_trial = np.array(mean_preds_per_trial)\n",
        "\n",
        "        pred_labels_per_trial = np.argmax(mean_preds_per_trial, axis=1)\n",
        "        assert pred_labels_per_trial.shape == dataset.y.shape\n",
        "        accuracy = np.mean(pred_labels_per_trial == dataset.y)\n",
        "        misclass = 1 - accuracy\n",
        "        column_name = \"{:s}_misclass\".format(setname)\n",
        "        out = {column_name: float(misclass)}\n",
        "        y = dataset.y\n",
        "\n",
        "        n_true_positive = np.sum((y == 1) & (pred_labels_per_trial == 1))\n",
        "        n_positive = np.sum(y == 1)\n",
        "        if n_positive > 0:\n",
        "            sensitivity = n_true_positive / float(n_positive)\n",
        "        else:\n",
        "            sensitivity = np.nan\n",
        "        column_name = \"{:s}_sensitivity\".format(setname)\n",
        "        out.update({column_name: float(sensitivity)})\n",
        "\n",
        "        n_true_negative = np.sum((y == 0) & (pred_labels_per_trial == 0))\n",
        "        n_negative = np.sum(y == 0)\n",
        "        if n_negative > 0:\n",
        "            specificity = n_true_negative / float(n_negative)\n",
        "        else:\n",
        "            specificity = np.nan\n",
        "        column_name = \"{:s}_specificity\".format(setname)\n",
        "        out.update({column_name: float(specificity)})\n",
        "        if (n_negative > 0) and (n_positive > 0):\n",
        "            auc = roc_auc_score(y, mean_preds_per_trial[:,1])\n",
        "        else:\n",
        "            auc = np.nan\n",
        "        column_name = \"{:s}_auc\".format(setname)\n",
        "        out.update({column_name: float(auc)})\n",
        "        return out\n",
        "\n",
        "def compute_preds_per_trial(preds_per_batch, dataset, input_time_length,\n",
        "                            n_stride):\n",
        "    n_trials = len(dataset.X)\n",
        "    i_pred_starts = [input_time_length -\n",
        "                     n_stride] * n_trials\n",
        "    i_pred_stops = [t.shape[1] for t in dataset.X]\n",
        "\n",
        "    start_stop_block_inds_per_trial = _compute_start_stop_block_inds(\n",
        "        i_pred_starts,\n",
        "        i_pred_stops, input_time_length, n_stride,\n",
        "        False)\n",
        "\n",
        "    n_rows_per_trial = [len(block_inds) for block_inds in\n",
        "                        start_stop_block_inds_per_trial]\n",
        "\n",
        "    all_preds_arr = np.concatenate(preds_per_batch, axis=0)\n",
        "    i_row = 0\n",
        "    preds_per_trial = []\n",
        "    for n_rows in n_rows_per_trial:\n",
        "        preds_per_trial.append(all_preds_arr[i_row:i_row + n_rows])\n",
        "        i_row += n_rows\n",
        "    assert i_row == len(all_preds_arr)\n",
        "    return preds_per_trial\n",
        "\n",
        "\n",
        "class CroppedNonDenseTrialMisclassMonitor(object):\n",
        "    \"\"\"\n",
        "    Compute trialwise misclasses from predictions for crops for non-dense predictions.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_time_length: int\n",
        "        Temporal length of one input to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_time_length, n_preds_per_input):\n",
        "        self.input_time_length = input_time_length\n",
        "        self.n_preds_per_input = n_preds_per_input\n",
        "\n",
        "    def monitor_epoch(self, ):\n",
        "        return\n",
        "\n",
        "    def monitor_set(self, setname, all_preds, all_losses,\n",
        "                    all_batch_sizes, all_targets, dataset):\n",
        "        \"\"\"Assuming one hot encoding for now\"\"\"\n",
        "        n_trials = len(dataset.X)\n",
        "        i_pred_starts = [self.input_time_length -\n",
        "                         self.n_preds_per_input] * n_trials\n",
        "        i_pred_stops = [t.shape[1] for t in dataset.X]\n",
        "\n",
        "        start_stop_block_inds_per_trial = _compute_start_stop_block_inds(\n",
        "            i_pred_starts,\n",
        "            i_pred_stops, self.input_time_length, self.n_preds_per_input,\n",
        "            False)\n",
        "\n",
        "        n_rows_per_trial = [len(block_inds) for block_inds in\n",
        "                            start_stop_block_inds_per_trial]\n",
        "\n",
        "        all_preds_arr = np.concatenate(all_preds, axis=0)\n",
        "        i_row = 0\n",
        "        preds_per_trial = []\n",
        "        for n_rows in n_rows_per_trial:\n",
        "            preds_per_trial.append(all_preds_arr[i_row:i_row + n_rows])\n",
        "            i_row += n_rows\n",
        "\n",
        "        mean_preds_per_trial = [np.mean(preds, axis=(0, 2)) for preds in\n",
        "                                preds_per_trial]\n",
        "        mean_preds_per_trial = np.array(mean_preds_per_trial)\n",
        "\n",
        "        pred_labels_per_trial = np.argmax(mean_preds_per_trial, axis=1)\n",
        "        assert pred_labels_per_trial.shape == dataset.y.shape\n",
        "        accuracy = np.mean(pred_labels_per_trial == dataset.y)\n",
        "        misclass = 1 - accuracy\n",
        "        column_name = \"{:s}_misclass\".format(setname)\n",
        "        return {column_name: float(misclass)}"
      ],
      "metadata": {
        "id": "H1YKRtNgW_U_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There should always be a 'train' and 'eval' folder directly\n",
        "# below these given folders\n",
        "# Folders should contain all normal and abnormal data files without duplications\n",
        "class Config():\n",
        "    data_folders = [\n",
        "        'drive/MyDrive/Data/normal/',\n",
        "        'drive/MyDrive/Data/abnormal/']\n",
        "    n_recordings = 600  # number of edf files to analyse, if you want to restrict the set size\n",
        "    sensor_types = [\"EEG\"]\n",
        "    n_chans = 21\n",
        "    max_recording_mins = 35  # exclude larger recordings from training set\n",
        "    sec_to_cut = 60  # cut away at start of each recording\n",
        "    duration_recording_mins = 5#20  # how many minutes to use per recording\n",
        "    test_recording_mins = 5#20\n",
        "    max_abs_val = 800  # for clipping\n",
        "    sampling_freq = 100\n",
        "    divisor = 10  # divide signal by this\n",
        "    test_on_eval = True  # teston evaluation set or on training set\n",
        "    # in case of test on eval, n_folds and i_testfold determine\n",
        "    # validation fold in training set for training until first stop\n",
        "    n_folds = 10\n",
        "    i_test_fold = 9\n",
        "    shuffle = True\n",
        "    model_name = 'deep'\n",
        "    n_start_chans = 25\n",
        "    n_chan_factor = 2  # relevant for deep model only\n",
        "    input_time_length = 30000\n",
        "    final_conv_length = 'auto'\n",
        "    model_constraint = 'defaultnorm'\n",
        "    init_lr = 1e-4\n",
        "    batch_size = 16\n",
        "    max_epochs = 50 # until first stop, the continue train on train+valid\n",
        "    cuda = True # False\n",
        "    n_classes = 2"
      ],
      "metadata": {
        "id": "asrBR6EjXCn1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import glob\n",
        "import os.path\n",
        "import mne\n",
        "\n",
        "def session_key(file_name):\n",
        "    \"\"\" sort the file name by session \"\"\"\n",
        "    return re.findall(r'(s\\d{2})', file_name)\n",
        "\n",
        "\n",
        "def natural_key(file_name):\n",
        "    \"\"\" provides a human-like sorting key of a string \"\"\"\n",
        "    key = [int(token) if token.isdigit() else None\n",
        "           for token in re.split(r'(\\d+)', file_name)]\n",
        "    return key\n",
        "\n",
        "def time_key(file_name):\n",
        "    \"\"\" provides a time-based sorting key \"\"\"\n",
        "    splits = file_name.split('/')\n",
        "    print(re.findall(r'(\\d{4}_\\d{2}_\\d{2})', splits[-2]))\n",
        "    [date] = re.findall(r'(\\d{4}_\\d{2}_\\d{2})', splits[-2])\n",
        "    date_id = [int(token) for token in date.split('_')]\n",
        "    recording_id = natural_key(splits[-1])\n",
        "    session_id = session_key(splits[-2])\n",
        "\n",
        "    return date_id + session_id + recording_id\n",
        "\n",
        "\n",
        "def read_all_file_names(path, extension, key=\"time\"):\n",
        "    \"\"\" read all files with specified extension from given path\n",
        "    :param path: parent directory holding the files directly or in subdirectories\n",
        "    :param extension: the type of the file, e.g. '.txt' or '.edf'\n",
        "    :param key: the sorting of the files. natural e.g. 1, 2, 12, 21 (machine 1, 12, 2, 21) or by time since this is\n",
        "    important for cv. time is specified in the edf file names\n",
        "    \"\"\"\n",
        "    file_paths = glob.glob(path + '**/*' + extension, recursive=True)\n",
        "\n",
        "    if key == 'time':\n",
        "        return sorted(file_paths, key=time_key)\n",
        "\n",
        "    elif key == 'natural':\n",
        "        return sorted(file_paths, key=natural_key)\n",
        "\n",
        "def get_info_with_mne(file_path):\n",
        "    \"\"\" read info from the edf file without loading the data. loading data is done in multiprocessing since it takes\n",
        "    some time. getting info is done before because some files had corrupted headers or weird sampling frequencies\n",
        "    that caused the multiprocessing workers to crash. therefore get and check e.g. sampling frequency and duration\n",
        "    beforehand\n",
        "    :param file_path: path of the recording file\n",
        "    :return: file name, sampling frequency, number of samples, number of signals, signal names, duration of the rec\n",
        "    \"\"\"\n",
        "    try:\n",
        "        edf_file = mne.io.read_raw_edf(file_path, verbose='error')\n",
        "    except ValueError:\n",
        "        return None, None, None, None, None, None\n",
        "        \n",
        "    # some recordings have a very weird sampling frequency. check twice before skipping the file\n",
        "    sampling_frequency = int(edf_file.info['sfreq'])\n",
        "    if sampling_frequency < 10:\n",
        "        sampling_frequency = 1 / (edf_file.times[1] - edf_file.times[0])\n",
        "        if sampling_frequency < 10:\n",
        "            return None, sampling_frequency, None, None, None, None\n",
        "\n",
        "    n_samples = edf_file.n_times\n",
        "    signal_names = edf_file.ch_names\n",
        "    n_signals = len(signal_names)\n",
        "    # some weird sampling frequencies are at 1 hz or below, which results in division by zero\n",
        "    duration = n_samples / max(sampling_frequency, 1)\n",
        "\n",
        "    # TODO: return rec object?\n",
        "    return edf_file, sampling_frequency, n_samples, n_signals, signal_names, duration\n",
        "\n",
        "\n",
        "def get_recording_length(file_path):\n",
        "    \"\"\" some recordings were that huge that simply opening them with mne caused the program to crash. therefore, open\n",
        "    the edf as bytes and only read the header. parse the duration from there and check if the file can safely be opened\n",
        "    :param file_path: path of the directory\n",
        "    :return: the duration of the recording\n",
        "    \"\"\"\n",
        "    f = open(file_path, 'rb')\n",
        "    header = f.read(256)\n",
        "    f.close()\n",
        "\n",
        "    return int(header[236:244].decode('ascii'))\n",
        "\n",
        "\n",
        "def load_data(fname, preproc_functions, sensor_types=['EEG']):\n",
        "    cnt, sfreq, n_samples, n_channels, chan_names, n_sec = get_info_with_mne(fname)\n",
        "    cnt.load_data()\n",
        "    selected_ch_names = []\n",
        "    \n",
        "    wanted_elecs = ['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1',\n",
        "                    'FP2', 'FZ', 'O1', 'O2',\n",
        "                    'P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
        "\n",
        "    for wanted_part in wanted_elecs:\n",
        "        wanted_found_name = []\n",
        "        for ch_name in cnt.ch_names:\n",
        "            if ' ' + wanted_part + '-' in ch_name:\n",
        "                wanted_found_name.append(ch_name)\n",
        "        selected_ch_names.append(wanted_found_name[0])\n",
        "\n",
        "\n",
        "    cnt = cnt.pick_channels(selected_ch_names)\n",
        "    n_sensors = 0\n",
        "    if 'EEG' in sensor_types:\n",
        "        n_sensors += 21\n",
        "    if 'EKG' in sensor_types:\n",
        "        n_sensors += 1\n",
        "\n",
        "    assert len(cnt.ch_names)  == n_sensors, (\n",
        "        \"Expected {:d} channel names, got {:d} channel names\".format(\n",
        "            n_sensors, len(cnt.ch_names)))\n",
        "\n",
        "    # change from volt to mikrovolt\n",
        "    data = (cnt.get_data() * 1e6).astype(np.float32)\n",
        "    fs = cnt.info['sfreq']\n",
        "    for fn in preproc_functions:\n",
        "        data, fs = fn(data, fs)\n",
        "        data = data.astype(np.float32)\n",
        "        fs = float(fs)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_all_sorted_file_names_and_labels(train_or_eval, folders):\n",
        "    all_file_names = []\n",
        "    for dirname, _, filenames in os.walk(folders[0]):\n",
        "        for filename in filenames:\n",
        "            all_file_names.append(os.path.join(dirname, filename))\n",
        "    for dirname, _, filenames in os.walk(folders[1]):\n",
        "        for filename in filenames:\n",
        "            all_file_names.append(os.path.join(dirname, filename))       \n",
        "    labels = ['/abnormal/' in f for f in all_file_names]\n",
        "    labels = np.array(labels).astype(np.int64)\n",
        "    return all_file_names, labels\n",
        "\n",
        "\n",
        "class DiagnosisSet(object):\n",
        "    def __init__(self, n_recordings, max_recording_mins, preproc_functions,\n",
        "                 data_folders,\n",
        "                 train_or_eval='train', sensor_types=['EEG'],):\n",
        "        self.n_recordings = n_recordings\n",
        "        self.max_recording_mins = max_recording_mins\n",
        "        self.preproc_functions = preproc_functions\n",
        "        self.train_or_eval = train_or_eval\n",
        "        self.sensor_types = sensor_types\n",
        "        self.data_folders = data_folders\n",
        "\n",
        "    def load(self, only_return_labels=False):\n",
        "        all_file_names, labels = get_all_sorted_file_names_and_labels(\n",
        "            train_or_eval=self.train_or_eval, folders=self.data_folders)\n",
        "        \n",
        "        if self.max_recording_mins is not None:\n",
        "            assert 'train' == self.train_or_eval\n",
        "            lengths = [get_recording_length(fname) for fname in all_file_names]\n",
        "            lengths = np.array(lengths)\n",
        "            mask = lengths < self.max_recording_mins * 60\n",
        "            cleaned_file_names = np.array(all_file_names)[mask]\n",
        "            cleaned_labels = labels[mask]\n",
        "        else:\n",
        "            cleaned_file_names = np.array(all_file_names)\n",
        "            cleaned_labels = labels\n",
        "        if only_return_labels:\n",
        "            return cleaned_labels\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        n_files = len(cleaned_file_names[:self.n_recordings])\n",
        "        for i_fname, fname in enumerate(tqdm(cleaned_file_names[:self.n_recordings])):\n",
        "            x = load_data(fname, preproc_functions=self.preproc_functions, sensor_types=self.sensor_types)\n",
        "            assert x is not None\n",
        "            X.append(x)\n",
        "            y.append(cleaned_labels[i_fname])\n",
        "            \n",
        "        y = np.array(y)\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "GAHZGJ2gXGZp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
        "from braindecode.models.deep4 import Deep4Net\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, output_size, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = 10\n",
        "        self.lstm = nn.LSTM(input_size=input_size[0], hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True, \n",
        "                            bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), \n",
        "            self.hidden_size))\n",
        "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), \n",
        "            self.hidden_size))\n",
        "\n",
        "        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        out = lstm_out[:, -1, :]\n",
        "        out = self.linear(out)\n",
        "    \n",
        "        return out\n",
        "        \n",
        "\n",
        "def build_model(config):\n",
        "    if config.model_name == 'shallow':\n",
        "        model = ShallowFBCSPNet(in_chans=config.n_chans, \n",
        "                                n_classes=config.n_classes,\n",
        "                                input_time_length=config.input_time_length,\n",
        "                                final_conv_length=config.final_conv_length).create_network()\n",
        "        \n",
        "    elif config.model_name == 'deep':\n",
        "        model = Deep4Net(in_chans=config.n_chans, n_classes=config.n_classes,\n",
        "                         #n_filters_time=config.n_start_chans,\n",
        "                         #n_filters_spat=config.n_start_chans,\n",
        "                         input_time_length=config.input_time_length,\n",
        "                         n_filters_2 = int(config.n_start_chans * config.n_chan_factor),\n",
        "                         n_filters_3 = int(config.n_start_chans * (config.n_chan_factor ** 2.0)),\n",
        "                         n_filters_4 = int(config.n_start_chans * (config.n_chan_factor ** 3.0)),\n",
        "                         final_conv_length=config.final_conv_length,\n",
        "                         stride_before_pool=True).create_network()\n",
        "        \n",
        "    elif (config.model_name == 'deep_smac'):\n",
        "        if config.model_name == 'deep_smac':\n",
        "            do_batch_norm = False\n",
        "        else:\n",
        "            assert config.model_name == 'deep_smac_bnorm'\n",
        "            do_batch_norm = True\n",
        "        double_time_convs = False\n",
        "        drop_prob = 0.244445\n",
        "        filter_length_2 = 12\n",
        "        filter_length_3 = 14\n",
        "        filter_length_4 = 12\n",
        "        filter_time_length = 21\n",
        "        final_conv_length = 1\n",
        "        first_nonlin = elu\n",
        "        first_pool_mode = 'mean'\n",
        "        first_pool_nonlin = identity\n",
        "        later_nonlin = elu\n",
        "        later_pool_mode = 'mean'\n",
        "        later_pool_nonlin = identity\n",
        "        n_filters_factor = 1.679066\n",
        "        n_filters_start = 32\n",
        "        pool_time_length = 1\n",
        "        pool_time_stride = 2\n",
        "        split_first_layer = True\n",
        "        n_chan_factor = n_filters_factor\n",
        "        n_start_chans = n_filters_start\n",
        "        model = Deep4Net(config.n_chans, config.n_classes,\n",
        "                 n_filters_time=n_start_chans,\n",
        "                 n_filters_spat=n_start_chans,\n",
        "                 input_time_length=config.input_time_length,\n",
        "                 n_filters_2=int(n_start_chans * n_chan_factor),\n",
        "                 n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
        "                 n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
        "                 final_conv_length=config.final_conv_length,\n",
        "                 batch_norm=do_batch_norm,\n",
        "                 double_time_convs=double_time_convs,\n",
        "                 drop_prob=drop_prob,\n",
        "                 filter_length_2=filter_length_2,\n",
        "                 filter_length_3=filter_length_3,\n",
        "                 filter_length_4=filter_length_4,\n",
        "                 filter_time_length=filter_time_length,\n",
        "                 first_nonlin=first_nonlin,\n",
        "                 first_pool_mode=first_pool_mode,\n",
        "                 first_pool_nonlin=first_pool_nonlin,\n",
        "                 later_nonlin=later_nonlin,\n",
        "                 later_pool_mode=later_pool_mode,\n",
        "                 later_pool_nonlin=later_pool_nonlin,\n",
        "                 pool_time_length=pool_time_length,\n",
        "                 pool_time_stride=pool_time_stride,\n",
        "                 split_first_layer=split_first_layer,\n",
        "                 stride_before_pool=True).create_network()\n",
        "        \n",
        "    elif config.model_name == 'shallow_smac':\n",
        "        conv_nonlin = identity\n",
        "        do_batch_norm = True\n",
        "        drop_prob = 0.328794\n",
        "        filter_time_length = 56\n",
        "        final_conv_length = 22\n",
        "        n_filters_spat = 73\n",
        "        n_filters_time = 24\n",
        "        pool_mode = 'max'\n",
        "        pool_nonlin = identity\n",
        "        pool_time_length = 84\n",
        "        pool_time_stride = 3\n",
        "        split_first_layer = True\n",
        "        model = ShallowFBCSPNet(in_chans=config.n_chans, n_classes=config.n_classes,\n",
        "                                n_filters_time=n_filters_time,\n",
        "                                n_filters_spat=n_filters_spat,\n",
        "                                input_time_length=input_time_length,\n",
        "                                final_conv_length=config.final_conv_length,\n",
        "                                conv_nonlin=conv_nonlin,\n",
        "                                batch_norm=do_batch_norm,\n",
        "                                drop_prob=drop_prob,\n",
        "                                filter_time_length=filter_time_length,\n",
        "                                pool_mode=pool_mode,\n",
        "                                pool_nonlin=pool_nonlin,\n",
        "                                pool_time_length=pool_time_length,\n",
        "                                pool_time_stride=pool_time_stride,\n",
        "                                split_first_layer=split_first_layer,\n",
        "                                ).create_network()\n",
        "    elif config.model_name == 'linear':\n",
        "        model = nn.Sequential()\n",
        "        model.add_module(\"conv_classifier\",\n",
        "                         nn.Conv2d(config.n_chans, config.n_classes, (600,1)))\n",
        "        model.add_module('Softmax', nn.Softmax(1))\n",
        "        model.add_module('squeeze', Expression(lambda x: x.squeeze(3)))\n",
        "        \n",
        "    #to_dense_prediction_model(model)\n",
        "    if config.cuda:\n",
        "        model.cuda()\n",
        "    return model  "
      ],
      "metadata": {
        "id": "iXguABpTXQSh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "__modifiedby__ = 'Mohamed Radwan'\n",
        "__originalauthor__ = 'David Nahmias'\n",
        "__credits__ = ['David Nahmias']\n",
        "__maintainer__ = 'Mohamed Radwan'\n",
        "\n",
        "import logging\n",
        "import time\n",
        "from copy import copy\n",
        "import sys\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "import resampy\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch as th\n",
        "from torch.nn.functional import elu\n",
        "from torch import nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from braindecode.datautil.signal_target import SignalAndTarget\n",
        "from braindecode.torch_ext.util import np_to_var\n",
        "from braindecode.torch_ext.util import set_random_seeds\n",
        "from braindecode.torch_ext.modules import Expression\n",
        "from braindecode.experiments.experiment import Experiment\n",
        "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
        "from braindecode.experiments.monitors import (RuntimeMonitor, LossMonitor,\n",
        "                                              MisclassMonitor)\n",
        "from braindecode.experiments.stopcriteria import MaxEpochs\n",
        "from braindecode.datautil.iterators import get_balanced_batches\n",
        "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
        "from braindecode.torch_ext.util import var_to_np\n",
        "from braindecode.torch_ext.functions import identity\n",
        "\n",
        "\n",
        "def splitDataRandom(allData,allLabels,setNum=0,shuffle=0):\n",
        "    numberEqSamples = min(Counter(allLabels).values())\n",
        "    trainSamplesNum = int(np.ceil(numberEqSamples*0.75))\n",
        "    testSamplesNum = numberEqSamples-trainSamplesNum\n",
        "\n",
        "    labels0 = allLabels[allLabels == 0]\n",
        "    labels1 = allLabels[allLabels == 1]\n",
        "    data0 = np.array(allData)[allLabels == 0]\n",
        "    data1 = np.array(allData)[allLabels == 1]\n",
        "\n",
        "    fullRange = list(range(numberEqSamples))\n",
        "    random.shuffle(fullRange)\n",
        "\n",
        "    testIndices = fullRange[trainSamplesNum:]\n",
        "    trainIndices = fullRange[:trainSamplesNum]\n",
        "    \n",
        "    allDataTrain = np.concatenate((data0[trainIndices],data1[trainIndices]),axis=0)\n",
        "    allLabelsTrain = np.concatenate((labels0[trainIndices],labels1[trainIndices]),axis=0)\n",
        "\n",
        "    allDataTest = np.concatenate((data0[testIndices],data1[testIndices]),axis=0)\n",
        "    allLabelsTest = np.concatenate((labels0[testIndices],labels1[testIndices]),axis=0)\n",
        "\n",
        "    return allDataTrain, allLabelsTrain, allDataTest, allLabelsTest\n",
        "\n",
        "\n",
        "def create_set(X, y, inds):\n",
        "    \"\"\"\n",
        "    X list and y nparray\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    new_X = []\n",
        "    for i in inds:\n",
        "        new_X.append(X[i])\n",
        "    new_y = y[inds]\n",
        "    return SignalAndTarget(new_X, new_y)\n",
        "\n",
        "\n",
        "class TrainValidTestSplitter(object):\n",
        "    def __init__(self, n_folds, i_test_fold, shuffle):\n",
        "        self.n_folds = n_folds\n",
        "        self.i_test_fold = i_test_fold\n",
        "        self.rng = RandomState(404)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def split(self, X, y,):\n",
        "        if len(X) < self.n_folds:\n",
        "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
        "                len(X), self.n_folds\n",
        "            ))\n",
        "        folds = get_balanced_batches(len(X), self.rng, self.shuffle,\n",
        "                                     n_batches=self.n_folds)\n",
        "        test_inds = folds[self.i_test_fold]\n",
        "        valid_inds = folds[self.i_test_fold - 1]\n",
        "        all_inds = list(range(len(X)))\n",
        "        train_inds = np.setdiff1d(all_inds, np.union1d(test_inds, valid_inds))\n",
        "        train_set = create_set(X, y, train_inds)\n",
        "        valid_set = create_set(X, y, valid_inds)\n",
        "        test_set = create_set(X, y, test_inds)\n",
        "\n",
        "        return train_set, valid_set, test_set\n",
        "\n",
        "\n",
        "class TrainValidSplitter(object):\n",
        "    def __init__(self, n_folds, i_valid_fold, shuffle):\n",
        "        self.n_folds = n_folds\n",
        "        self.i_valid_fold = i_valid_fold\n",
        "        self.rng = RandomState(404)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def split(self, X, y):\n",
        "        folds = get_balanced_batches(len(X), self.rng, self.shuffle,\n",
        "                                     n_batches=self.n_folds)\n",
        "        valid_inds = folds[self.i_valid_fold]\n",
        "        all_inds = list(range(len(X)))\n",
        "        train_inds = np.setdiff1d(all_inds, valid_inds)\n",
        "        train_set = create_set(X, y, train_inds)\n",
        "        valid_set = create_set(X, y, valid_inds)\n",
        "        return train_set, valid_set\n",
        "\n",
        "    \n",
        "def preprocess(config):\n",
        "    preproc_functions = []\n",
        "    preproc_functions.append(\n",
        "        lambda data, fs: (data[:, int(config.sec_to_cut * fs):-int(\n",
        "            config.sec_to_cut * fs)], fs))\n",
        "    preproc_functions.append(\n",
        "        lambda data, fs: (data[:, :int(config.duration_recording_mins * 60 * fs)], fs))\n",
        "    if config.max_abs_val is not None:\n",
        "        preproc_functions.append(lambda data, fs:\n",
        "                                 (np.clip(data, -config.max_abs_val, config.max_abs_val), fs))\n",
        "\n",
        "    preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,\n",
        "                                                                config.sampling_freq,\n",
        "                                                                axis=1,\n",
        "                                                                filter='kaiser_fast'),\n",
        "                                               config.sampling_freq))\n",
        "\n",
        "    if config.divisor is not None:\n",
        "        preproc_functions.append(lambda data, fs: (data / config.divisor, fs))\n",
        "        \n",
        "    return preproc_functions\n",
        "\n",
        "    \n",
        "def data_loader(config):\n",
        "    cudnn.benchmark = True\n",
        "    \n",
        "    preproc_functions = preprocess(config)\n",
        "    \n",
        "    dataset = DiagnosisSet(n_recordings=config.n_recordings,\n",
        "                           max_recording_mins=config.max_recording_mins,\n",
        "                           preproc_functions=preproc_functions,\n",
        "                           data_folders=config.data_folders,\n",
        "                           train_or_eval='train',\n",
        "                           sensor_types=config.sensor_types)\n",
        "    \n",
        "    if config.test_on_eval:\n",
        "        test_recording_mins = config.duration_recording_mins\n",
        "        test_preproc_functions = copy(preproc_functions)\n",
        "        test_preproc_functions[1] = lambda data, fs: (\n",
        "            data[:, :int(test_recording_mins * 60 * fs)], fs)\n",
        "        test_dataset = DiagnosisSet(n_recordings=config.n_recordings,\n",
        "                                max_recording_mins=None,\n",
        "                                preproc_functions=test_preproc_functions,\n",
        "                                data_folders=config.data_folders,\n",
        "                                train_or_eval='val',\n",
        "                                sensor_types=config.sensor_types)\n",
        "\n",
        "    data, labels = dataset.load()\n",
        "    X,y,test_X,test_y = splitDataRandom(data, labels,shuffle=0)\n",
        "    \n",
        "    if config.test_on_eval:\n",
        "        max_shape = np.max([list(x.shape) for x in test_X], axis=0)\n",
        "    if not config.test_on_eval:\n",
        "        splitter = TrainValidTestSplitter(config.n_folds, config.i_test_fold,\n",
        "                                          shuffle=config.shuffle)\n",
        "        train_set, valid_set, test_set = splitter.split(X, y)\n",
        "        \n",
        "    else:\n",
        "        splitter = TrainValidSplitter(config.n_folds, \n",
        "                                      i_valid_fold=config.i_test_fold,\n",
        "                                      shuffle=config.shuffle)\n",
        "        train_set, valid_set = splitter.split(X, y)\n",
        "        test_set = SignalAndTarget(test_X, test_y)\n",
        "        \n",
        "    return train_set, valid_set, test_set"
      ],
      "metadata": {
        "id": "F6jMZBPYXWh7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "train_set, valid_set, test_set = data_loader(config)\n",
        "\n",
        "model = build_model(config)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.init_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAtY33OQXnK1",
        "outputId": "66ae068b-05b3-4507-e242-08543f25c239"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 567/567 [10:38<00:00,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
        "from braindecode.datautil.iterators import get_balanced_batches\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rng = RandomState((2018,8,7))\n",
        "\n",
        "\n",
        "def evaluate_on_val(model, data, config):\n",
        "    \"\"\"print metrics each epoch\"\"\"\n",
        "    model.eval()\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    i_trials_in_batch = get_balanced_batches(len(data.X), rng, \n",
        "                                             shuffle=True, \n",
        "                                             batch_size=config.batch_size)\n",
        "    for i_trials in i_trials_in_batch:\n",
        "        batch_X = np.array(data.X)[i_trials][:,:,:,None]\n",
        "        batch_y = np.array(data.y)[i_trials]\n",
        "        batch_X = np_to_var(batch_X)   \n",
        "        batch_y = np_to_var(batch_y)     \n",
        "        if config.cuda:\n",
        "            batch_X = batch_X.cuda()\n",
        "            batch_y = batch_y.cuda()\n",
        "        outputs = model(batch_X)\n",
        "        loss = F.nll_loss(outputs, batch_y)\n",
        "        outputs = outputs.cpu().detach().numpy()\n",
        "        batch_y = batch_y.cpu().detach().numpy()\n",
        "        predicted_labels = np.argmax(outputs, axis=1)\n",
        "        accuracy = accuracy_score(batch_y, predicted_labels)\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    print('Accuracy: ', np.mean(accuracies), ', Loss: ', np.mean(losses))\n",
        "    \n",
        "\n",
        "def evaluate_on_test(model, eeg_features, labels, config):\n",
        "    \"\"\"evaluate the model on test data\"\"\"\n",
        "    accuracies = []\n",
        "    model.eval()\n",
        "    i_trials_in_batch = get_balanced_batches(len(eeg_features), rng, \n",
        "                                                 shuffle=True,\n",
        "                                                 batch_size=config.batch_size)\n",
        "    for i_trials in i_trials_in_batch:\n",
        "        batch_X = np.array(eeg_features)[i_trials][:,:,:,None]\n",
        "        batch_y = np.array(labels)[i_trials]\n",
        "        batch_y = np_to_var(batch_y) \n",
        "        batch_X = np_to_var(batch_X) \n",
        "        if config.cuda:\n",
        "            batch_X = batch_X.cuda()\n",
        "            batch_y = batch_y.cuda()\n",
        "        outputs = model(batch_X)\n",
        "        outputs = outputs.cpu().detach().numpy()\n",
        "        batch_y = batch_y.cpu().detach().numpy()\n",
        "        predicted_labels = np.argmax(outputs, axis=1)\n",
        "        accuracy = accuracy_score(batch_y, predicted_labels)\n",
        "        accuracies.append(accuracy)\n",
        "    print('Test Accuracy: ', np.mean(accuracies))\n",
        "\n",
        "\n",
        "def train(config, model, optimizer, train_set, valid_set):\n",
        "    for i_epoch in range(1, config.max_epochs):\n",
        "        i_trials_in_batch = get_balanced_batches(len(train_set.X), rng, \n",
        "                                                 shuffle=True,\n",
        "                                                 batch_size=config.batch_size)\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        for i_trials in i_trials_in_batch:\n",
        "        # Have to add empty fourth dimension to X\n",
        "        \n",
        "            batch_X = np.array(train_set.X)[i_trials][:,:,:,None]\n",
        "            batch_y = np.array(train_set.y)[i_trials]\n",
        "            batch_X = np_to_var(batch_X)\n",
        "            if config.cuda:\n",
        "                batch_X = batch_X.cuda()\n",
        "            batch_y = np_to_var(batch_y)\n",
        "            if config.cuda:\n",
        "                batch_y = batch_y.cuda()\n",
        "            # Remove gradients of last backward pass from all parameters\n",
        "            optimizer.zero_grad()\n",
        "            # Compute outputs of the network\n",
        "            outputs = model(batch_X)\n",
        "            # Compute the loss\n",
        "            loss = F.nll_loss(outputs, batch_y)\n",
        "            # Do the backpropagation\n",
        "            loss.backward()\n",
        "            # Update parameters with the optimizer\n",
        "            optimizer.step()\n",
        "        print('Epoch ', i_epoch)\n",
        "        print('========')\n",
        "        print('Training Metrics: ')\n",
        "        evaluate_on_val(model, train_set, config)\n",
        "        print('Validation Metrics: ')\n",
        "        evaluate_on_val(model, valid_set, config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "QJEkSMl1YMrL"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep4Net Model"
      ],
      "metadata": {
        "id": "NRkU0qt3a4mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "model = build_model(config)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.init_lr)\n",
        "model = train(config, model, optimizer, train_set, valid_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B1qf5VhZtFh",
        "outputId": "fa406a55-e39e-4c71-95dc-cd61ca95dfad"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.527536231884058 , Loss:  1.4153133086536243\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.747484266757965\n",
            "Epoch  2\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5626811594202898 , Loss:  0.6958337400270544\n",
            "Validation Metrics: \n",
            "Accuracy:  0.475 , Loss:  0.8347907364368439\n",
            "Epoch  3\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5166666666666666 , Loss:  1.3553521918213887\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.6935781240463257\n",
            "Epoch  4\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5166666666666666 , Loss:  1.0842221949411475\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.410382628440857\n",
            "Epoch  5\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5291666666666667 , Loss:  1.0384441717811252\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.4167568683624268\n",
            "Epoch  6\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5576086956521739 , Loss:  0.7346386935399927\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.030613511800766\n",
            "Epoch  7\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5358695652173913 , Loss:  0.8194335530633512\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.1264616250991821\n",
            "Epoch  8\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5230072463768116 , Loss:  0.9680662194023961\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.3929715752601624\n",
            "Epoch  9\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5634057971014492 , Loss:  0.7677963078022003\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.1400994658470154\n",
            "Epoch  10\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5501811594202899 , Loss:  0.8450759571531544\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.2059972286224365\n",
            "Epoch  11\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5605072463768116 , Loss:  0.9059555556463159\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.4383267760276794\n",
            "Epoch  12\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5519927536231883 , Loss:  0.9572679633679597\n",
            "Validation Metrics: \n",
            "Accuracy:  0.44999999999999996 , Loss:  1.5428841710090637\n",
            "Epoch  13\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5304347826086956 , Loss:  1.3802827544834302\n",
            "Validation Metrics: \n",
            "Accuracy:  0.45 , Loss:  1.934212565422058\n",
            "Epoch  14\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5873188405797102 , Loss:  0.7674493530522222\n",
            "Validation Metrics: \n",
            "Accuracy:  0.475 , Loss:  1.2644650340080261\n",
            "Epoch  15\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.6164855072463769 , Loss:  0.610799675402434\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.0930263996124268\n",
            "Epoch  16\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5632246376811594 , Loss:  0.8014885653620181\n",
            "Validation Metrics: \n",
            "Accuracy:  0.475 , Loss:  1.3170042037963867\n",
            "Epoch  17\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.6126811594202899 , Loss:  0.7224814982517905\n",
            "Validation Metrics: \n",
            "Accuracy:  0.5 , Loss:  1.3768892884254456\n",
            "Epoch  18\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.621195652173913 , Loss:  0.7630982528562131\n",
            "Validation Metrics: \n",
            "Accuracy:  0.5 , Loss:  1.5325725078582764\n",
            "Epoch  19\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.6632246376811594 , Loss:  0.6033588932908099\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.3754454255104065\n",
            "Epoch  20\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5570652173913043 , Loss:  0.9765107968579168\n",
            "Validation Metrics: \n",
            "Accuracy:  0.475 , Loss:  1.6813774704933167\n",
            "Epoch  21\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.592391304347826 , Loss:  0.678235966226329\n",
            "Validation Metrics: \n",
            "Accuracy:  0.475 , Loss:  1.2116641998291016\n",
            "Epoch  22\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.742572463768116 , Loss:  0.5122464435256046\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.1950843930244446\n",
            "Epoch  23\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.6967391304347825 , Loss:  0.5458344039709672\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.1298270225524902\n",
            "Epoch  24\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.5898550724637681 , Loss:  0.9438298603762751\n",
            "Validation Metrics: \n",
            "Accuracy:  0.5 , Loss:  1.911238193511963\n",
            "Epoch  25\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7862318840579711 , Loss:  0.40869909881249716\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.2128167748451233\n",
            "Epoch  26\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7028985507246376 , Loss:  0.5935603457948436\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.4754258394241333\n",
            "Epoch  27\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7317028985507248 , Loss:  0.48864933459655097\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.4276850819587708\n",
            "Epoch  28\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7817028985507247 , Loss:  0.4392145619444225\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.420452892780304\n",
            "Epoch  29\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.6418478260869565 , Loss:  0.7318677513495736\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.6088120937347412\n",
            "Epoch  30\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7346014492753624 , Loss:  0.5306748268396958\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.5628772675991058\n",
            "Epoch  31\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8061594202898551 , Loss:  0.3441968709230423\n",
            "Validation Metrics: \n",
            "Accuracy:  0.525 , Loss:  1.1579803824424744\n",
            "Epoch  32\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7978260869565218 , Loss:  0.3883670924798302\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.4490099251270294\n",
            "Epoch  33\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8494565217391304 , Loss:  0.27111222044281336\n",
            "Validation Metrics: \n",
            "Accuracy:  0.625 , Loss:  1.2621780931949615\n",
            "Epoch  34\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8527173913043479 , Loss:  0.2965001089741354\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.276547759771347\n",
            "Epoch  35\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7867753623188406 , Loss:  0.3881532549858093\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.4407545626163483\n",
            "Epoch  36\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8684782608695653 , Loss:  0.25133655673783756\n",
            "Validation Metrics: \n",
            "Accuracy:  0.6 , Loss:  1.2555044889450073\n",
            "Epoch  37\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7434782608695653 , Loss:  0.4691813682084498\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.3915805220603943\n",
            "Epoch  38\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7077898550724637 , Loss:  0.5698382446299428\n",
            "Validation Metrics: \n",
            "Accuracy:  0.55 , Loss:  1.6461536288261414\n",
            "Epoch  39\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8518115942028986 , Loss:  0.27341353115828143\n",
            "Validation Metrics: \n",
            "Accuracy:  0.6 , Loss:  1.160519540309906\n",
            "Epoch  40\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8521739130434783 , Loss:  0.28098650406236236\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.2693706154823303\n",
            "Epoch  41\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8112318840579711 , Loss:  0.3448271398311076\n",
            "Validation Metrics: \n",
            "Accuracy:  0.6000000000000001 , Loss:  1.6594507098197937\n",
            "Epoch  42\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.756340579710145 , Loss:  0.46691441859887994\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.596401572227478\n",
            "Epoch  43\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7572463768115942 , Loss:  0.48626466872899426\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.6986277103424072\n",
            "Epoch  44\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8273550724637682 , Loss:  0.31047452759483585\n",
            "Validation Metrics: \n",
            "Accuracy:  0.6 , Loss:  1.171884298324585\n",
            "Epoch  45\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.855072463768116 , Loss:  0.2633071948328744\n",
            "Validation Metrics: \n",
            "Accuracy:  0.625 , Loss:  1.2342578768730164\n",
            "Epoch  46\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8331521739130435 , Loss:  0.3288620020384374\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.3788574934005737\n",
            "Epoch  47\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.8610507246376812 , Loss:  0.25609894744727923\n",
            "Validation Metrics: \n",
            "Accuracy:  0.625 , Loss:  1.4688382744789124\n",
            "Epoch  48\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.9481884057971015 , Loss:  0.1968426923065082\n",
            "Validation Metrics: \n",
            "Accuracy:  0.625 , Loss:  1.1516525745391846\n",
            "Epoch  49\n",
            "========\n",
            "Training Metrics: \n",
            "Accuracy:  0.7760869565217392 , Loss:  0.43296445323073346\n",
            "Validation Metrics: \n",
            "Accuracy:  0.575 , Loss:  1.6059702038764954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"in the original implimentation, the author start a new array as zeros,\n",
        "and then add perturbations to the zeros array, to show the spectral importance.\n",
        "Here, we use the same original data nad only perturb the required spectrum \n",
        "one at a time\"\"\"\n",
        "from scipy import fftpack\n",
        "import copy\n",
        "\n",
        "def addDataNoise(origSignals,band=[],channels=[],srate=100, zeroing=False):\n",
        "    np.random.seed(seed=404)\n",
        "    signals = copy.deepcopy(origSignals)\n",
        "\n",
        "    if (len(band)+len(channels)) == 0:\n",
        "        return origSignals\n",
        "    \n",
        "    if (len(channels)>0) and (len(band)==0):\n",
        "        for s in range(len(signals)):\n",
        "            for c in channels:\n",
        "                cleanSignal = origSignals[s][c,:]\n",
        "                timeDomNoise = np.random.normal(np.mean(cleanSignal), \n",
        "                                                np.std(cleanSignal), \n",
        "                                                size=len(cleanSignal))\n",
        "                signals[s][c,:] = np.float32(timeDomNoise)\n",
        "                \"\"\"add noise to all channels: cleanSignal + timeDomNoise\"\"\"\n",
        "\n",
        "    if (len(band) == 2) and (type(band[0]) == int):\n",
        "        if len(channels)==0:\n",
        "            channels = range(signals[0].shape[0])\n",
        "        numSamples = signals[0].shape[1]\n",
        "        W = fftpack.rfftfreq(numSamples,d=1./srate)\n",
        "        lowHz = next(x[0] for x in enumerate(W) if x[1] > band[0])\n",
        "        highHz = next(x[0] for x in enumerate(W) if x[1] > band[1])\n",
        "        for s in range(len(signals)):\n",
        "            for c in channels: #loop through channels\n",
        "                dataDFT = fftpack.rfft(origSignals[s][c,:])\n",
        "                cleanDFT = dataDFT[lowHz:highHz]\n",
        "                freqDomNoise = np.random.normal(np.mean(cleanDFT), \n",
        "                                                np.std(cleanDFT), \n",
        "                                                size=len(cleanDFT))\n",
        "                dataDFT[lowHz:highHz] =  freqDomNoise#cleanDFT + freqDomNoise\n",
        "                signals[s][c,:] = np.float32(fftpack.irfft(dataDFT))\n",
        "\n",
        "\n",
        "    elif (len(band)>0) and (type(band) == list):\n",
        "        if len(channels)==0:\n",
        "            channels = range(origSignals[0].shape[0])\n",
        "        \n",
        "        numSamples = origSignals[0].shape[1]\n",
        "        W = fftpack.rfftfreq(numSamples, d=1./srate)    \n",
        "        \n",
        "        for s in range(len(signals)):\n",
        "            for c in channels: #loop through channels\n",
        "                dataDFT_original = fftpack.rfft(origSignals[s][c,:])\n",
        "                dataDFT_output = fftpack.rfft(signals[s][c,:])\n",
        "                for b in band:\n",
        "                    lowHz = next(x[0] for x in enumerate(W) if x[1] > b[0])\n",
        "                    highHz = next(x[0] for x in enumerate(W) if x[1] > b[1])\n",
        "                    cleanDFT = dataDFT_original[lowHz:highHz]\n",
        "                    freqDomNoise = np.random.normal(np.mean(cleanDFT), \n",
        "                                                    np.std(cleanDFT), \n",
        "                                                    size=len(cleanDFT))\n",
        "                    if zeroing:\n",
        "                        dataDFT_output[lowHz:highHz] =  0 #no signal in this frequency\n",
        "                    else:\n",
        "                        dataDFT_output[lowHz:highHz] = freqDomNoise #freqDomNoise\n",
        "                signals[s][c,:] = np.float32(fftpack.irfft(dataDFT_output))\n",
        "\n",
        "    return signals"
      ],
      "metadata": {
        "id": "H6SVYV6Fj4Sl"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Data using spectral removing (remove frequencey band) and perturbation"
      ],
      "metadata": {
        "id": "bqW-961bSXQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta = [[0, 4]]\n",
        "theta = [[4, 8]]\n",
        "alpha = [[8, 12]]\n",
        "mu = [[12, 16]]\n",
        "beta = [[16, 25]]\n",
        "gamma = [[25, 40]]\n",
        "\n",
        "print('No removal of spectral data')\n",
        "evaluate_on_test(model, test_set.X, test_set.y, config)\n",
        "\n",
        "print('Delta band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=delta, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Theta band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=theta, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Alpha band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=alpha, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Mu band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=mu, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Beta band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=beta, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Gamma band removed')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=gamma, srate=100, zeroing=True)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)"
      ],
      "metadata": {
        "id": "_F7JHsoNukmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ffa3e4-eeb5-4ecd-e33a-7841a8753992"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No perturbation\n",
            "Test Accuracy:  0.6415441176470589\n",
            "Delta band removed\n",
            "Test Accuracy:  0.5303308823529411\n",
            "Theta band removed\n",
            "Test Accuracy:  0.6194852941176471\n",
            "Alpha band removed\n",
            "Test Accuracy:  0.6397058823529411\n",
            "Mu band removed\n",
            "Test Accuracy:  0.6351102941176471\n",
            "Beta band removed\n",
            "Test Accuracy:  0.6346507352941176\n",
            "Gamma band removed\n",
            "Test Accuracy:  0.6424632352941176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('No perturbation')\n",
        "evaluate_on_test(model, test_set.X, test_set.y, config)\n",
        "\n",
        "print('Delta band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=delta, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Theta band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=theta, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Alpha band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=alpha, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Mu band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=mu, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Beta band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=beta, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)\n",
        "\n",
        "print('Gamma band perturbation')\n",
        "test_noisy_band = addDataNoise(test_set.X, band=gamma, srate=100)\n",
        "evaluate_on_test(model, test_noisy_band, test_set.y, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6qAeezzutIZ",
        "outputId": "cbec9a5c-aa77-4950-ab8c-2fe3c1ba2224"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No perturbation\n",
            "Test Accuracy:  0.6443014705882353\n",
            "Delta band perturbation\n",
            "Test Accuracy:  0.6796875\n",
            "Theta band perturbation\n",
            "Test Accuracy:  0.6259191176470589\n",
            "Alpha band perturbation\n",
            "Test Accuracy:  0.6484375\n",
            "Mu band perturbation\n",
            "Test Accuracy:  0.6351102941176471\n",
            "Beta band perturbation\n",
            "Test Accuracy:  0.6332720588235294\n",
            "Gamma band perturbation\n",
            "Test Accuracy:  0.6433823529411764\n"
          ]
        }
      ]
    }
  ]
}